"""
èµ°å‹¢åˆ†ææ¨¡çµ„ - ä½¿ç”¨Google Vertex AIçš„Geminiæ¨¡å‹åˆ†æå¸‚å ´èµ°å‹¢
"""
import os
import pandas as pd
import json
import time
from datetime import datetime, timedelta # Ensure timedelta is imported
from typing import Dict, Any, Optional
import traceback
import numpy as np # Ensure numpy is imported
import re # Ensure re is imported for mock response generation

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # dotenv æ˜¯å¯é¸çš„

try:
    from google.cloud import aiplatform
    from google.oauth2 import service_account
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False
    print("è­¦å‘Š: Google Cloud AI Platform ä¾è³´æœªå®‰è£ã€‚è«‹å®‰è£ google-cloud-aiplatform å’Œ google-generativeai")

class TrendAnalyzer:
    """ä½¿ç”¨Geminiæ¨¡å‹åˆ†æå¸‚å ´èµ°å‹¢çš„é¡"""

    def __init__(self, api_key: Optional[str] = None, project_id: Optional[str] = None, location: Optional[str] = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            api_key: Google APIå¯†é‘° (å¦‚æœæœªæä¾›ï¼Œå°‡å¾ç’°å¢ƒè®Šæ•¸ GOOGLE_API_KEY è®€å–)
            project_id: Google Cloudå°ˆæ¡ˆID (å¦‚æœæœªæä¾›ï¼Œå°‡å¾ç’°å¢ƒè®Šæ•¸ GOOGLE_PROJECT_ID è®€å–)
            location: Vertex AIä½ç½® (å¦‚æœæœªæä¾›ï¼Œå°‡å¾ç’°å¢ƒè®Šæ•¸ GOOGLE_LOCATION è®€å–ï¼Œé è¨­ç‚º us-central1)
        """
        if not GOOGLE_AVAILABLE:
            raise ImportError("Google Cloud AI Platform ä¾è³´æœªå®‰è£ã€‚è«‹å®‰è£ç›¸é—œå¥—ä»¶ã€‚")

        # å¾åƒæ•¸æˆ–ç’°å¢ƒè®Šæ•¸ç²å–é…ç½®
        self.api_key = api_key or os.environ.get("GOOGLE_API_KEY")
        self.project_id = project_id or os.environ.get("GOOGLE_PROJECT_ID")
        self.location = location or os.environ.get("GOOGLE_LOCATION", "us-central1")

        # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„é…ç½®
        if not self.api_key and not self.project_id:
            print("è­¦å‘Š: æœªæ‰¾åˆ°Google APIå¯†é‘°æˆ–å°ˆæ¡ˆID")
            print("è«‹åœ¨ç’°å¢ƒè®Šæ•¸ä¸­è¨­ç½® GOOGLE_API_KEY æˆ– GOOGLE_PROJECT_ID")
            print("æˆ–è€…åœ¨åˆå§‹åŒ–æ™‚æä¾›é€™äº›åƒæ•¸")

        self.model = None
        self.max_retries = 3
        self.retry_delay = 2

        self._init_ai_client()

    def _init_ai_client(self):
        """åˆå§‹åŒ–AIå®¢æˆ¶ç«¯é€£æ¥"""
        try:
            # å„ªå…ˆä½¿ç”¨Google Generative AI (æ›´ç°¡å–®çš„API)
            if self.api_key:
                genai.configure(api_key=self.api_key)
                self.model = genai.GenerativeModel('gemini-1.5-flash')
                print("å·²åˆå§‹åŒ– Google Generative AI å®¢æˆ¶ç«¯")
                return

            # å‚™ç”¨ï¼šä½¿ç”¨Vertex AI
            if self.project_id:
                # å¦‚æœæœ‰æœå‹™å¸³è™Ÿé‡‘é‘°æ–‡ä»¶ï¼Œä½¿ç”¨å®ƒ
                if os.path.exists("google_credentials.json"):
                    credentials = service_account.Credentials.from_service_account_file(
                        "google_credentials.json"
                    )
                    aiplatform.init(
                        project=self.project_id,
                        location=self.location,
                        credentials=credentials
                    )
                else:
                    aiplatform.init(project=self.project_id, location=self.location)

                self.model = aiplatform.GenerativeModel("gemini-1.5-flash")
                print("å·²åˆå§‹åŒ– Vertex AI å®¢æˆ¶ç«¯")
                return

            raise ValueError("éœ€è¦æä¾›Google APIå¯†é‘°æˆ–Google Cloudå°ˆæ¡ˆID")

        except Exception as e:
            print(f"åˆå§‹åŒ–AIå®¢æˆ¶ç«¯æ™‚å‡ºéŒ¯: {e}")
            traceback.print_exc()
            raise

    def analyze_trend(self, data: Optional[pd.DataFrame], symbol: str, timeframe: str, detail_level: str = "æ¨™æº–") -> Dict[str, Any]:
        """
        N8Nå·¥ä½œæµå®Œæ•´ç§»æ¤ - å°ˆæ¥­ç´šåŠ å¯†è²¨å¹£åˆ†æç³»çµ±

        å®Œæ•´è¤‡è£½N8Nå·¥ä½œæµçš„åˆ†æé‚è¼¯ï¼š
        1. ç²å–å¤šæ™‚é–“æ¡†æ¶Kç·šæ•¸æ“š (15m, 1h, 1d)
        2. ç²å–ä¸¦åˆ†æåŠ å¯†è²¨å¹£æ–°èæƒ…ç·’
        3. ä½¿ç”¨Google Geminié€²è¡Œç¶œåˆæŠ€è¡“åˆ†æ
        4. ç”Ÿæˆå…·é«”çš„ç¾è²¨å’Œæ§“æ¡¿äº¤æ˜“å»ºè­°

        Args:
            data: åŒ…å«OHLCVæ•¸æ“šçš„DataFrame (å¯é¸, åœ¨N8Næ¨¡å¼ä¸‹ç‚ºNone)
            symbol: äº¤æ˜“å°ç¬¦è™Ÿ
            timeframe: æ™‚é–“æ¡†æ¶
            detail_level: åˆ†æè©³ç´°ç¨‹åº¦ ("ç°¡è¦", "æ¨™æº–", "è©³ç´°")

        Returns:
            åˆ†æçµæœå­—å…¸
        """
        try:
            print(f"ğŸš€ é–‹å§‹N8Nå·¥ä½œæµåˆ†æ {symbol} {timeframe} æ•¸æ“š...")

            if data is not None and not self._validate_data(data): # åƒ…åœ¨æä¾›äº†dataæ™‚é©—è­‰
                    raise ValueError("æ•¸æ“šé©—è­‰å¤±æ•—")

            # æ­¥é©Ÿ1: ç²å–å¤šæ™‚é–“æ¡†æ¶Kç·šæ•¸æ“š
            print("ğŸ“Š æ­¥é©Ÿ1: ç²å–å¤šæ™‚é–“æ¡†æ¶Kç·šæ•¸æ“š...")
            multi_timeframe_data = self._fetch_multi_timeframe_data(symbol)

            # æ­¥é©Ÿ2: ç²å–ä¸¦åˆ†ææ–°èæƒ…ç·’
            print("ğŸ“° æ­¥é©Ÿ2: ç²å–ä¸¦åˆ†ææ–°èæƒ…ç·’...")
            news_sentiment = self._fetch_and_analyze_news_sentiment(symbol)

            # æ­¥é©Ÿ3: åˆä½µæ‰€æœ‰æ•¸æ“š
            print("ğŸ”„ æ­¥é©Ÿ3: åˆä½µæŠ€è¡“æ•¸æ“šå’Œæƒ…ç·’æ•¸æ“š...")
            combined_data = self._combine_technical_and_sentiment_data(
                multi_timeframe_data, news_sentiment
            )

            # æ­¥é©Ÿ4: ä½¿ç”¨Google Geminié€²è¡Œå°ˆæ¥­åˆ†æ
            print("ğŸ¯ æ­¥é©Ÿ4: ä½¿ç”¨Google Geminié€²è¡Œå°ˆæ¥­åˆ†æ...")
            professional_analysis = self._generate_professional_trading_analysis(
                symbol, combined_data, detail_level
            )

            return professional_analysis

        except Exception as e:
            error_msg = f"åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            print(error_msg)
            traceback.print_exc()
            return {
                "analysis_text": error_msg,
                "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "symbol": symbol,
                "timeframe": timeframe,
                "status": "error_formatting"
            }

    def _fetch_multi_timeframe_data(self, symbol: str) -> Dict[str, Any]:
        """æ­¥é©Ÿ1: ç²å–å¤šæ™‚é–“æ¡†æ¶Kç·šæ•¸æ“š (æ¨¡æ“¬N8Nçš„HTTPè«‹æ±‚)"""
        try:
            # import requests # Keep this if actual API calls are made
            # import numpy as np # Already imported globally

            timeframes = ['15m', '1h', '1d']
            all_candles_data = [] # Renamed for clarity

            for tf in timeframes:
                print(f"   ç²å– {symbol} {tf} Kç·šæ•¸æ“š...")
                candles_data = self._generate_realistic_kline_data(symbol, tf, 200)
                formatted_data = {
                    "timeframe": tf,
                    "candles": candles_data
                }
                all_candles_data.append(formatted_data)
                print(f"   âœ… {tf} æ•¸æ“šç²å–å®Œæˆ ({len(candles_data)} æ ¹Kç·š)")

            return {
                "allCandles": all_candles_data,
                "symbol": symbol,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            print(f"ç²å–å¤šæ™‚é–“æ¡†æ¶æ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
            return {
                "error": str(e),
                "allCandles": [],
                "symbol": symbol, # Include symbol even in error
                "timestamp": datetime.now().isoformat()
            }

    def _generate_realistic_kline_data(self, symbol: str, timeframe: str, limit: int) -> list:
        """ç”Ÿæˆæ›´çœŸå¯¦çš„Kç·šæ•¸æ“šæ ¼å¼ (æ¨¡æ“¬Binance APIå›æ‡‰)"""
        try:
            # åŸºç¤åƒ¹æ ¼ç¯„åœè¨­å®š
            symbol_price_ranges = {
                "BTC": (60000.0, 75000.0), "ETH": (3000.0, 4000.0),
                "SUI": (0.8, 1.5), "SOL": (150.0, 200.0),
                "ADA": (0.4, 0.6), "DOT": (5.0, 8.0),
                "DOGE": (0.1, 0.2), "LINK": (15.0, 25.0),
                "DEFAULT": (1.0, 50.0) # Adjusted default for typical altcoins
            }
            base_volatility = 0.025 # Slightly increased base volatility

            selected_range = symbol_price_ranges.get(symbol.upper(), symbol_price_ranges["DEFAULT"])
            for prefix, price_range in symbol_price_ranges.items():
                if symbol.upper().startswith(prefix):
                    selected_range = price_range
                    break
            base_price = np.random.uniform(selected_range[0], selected_range[1])

            interval_minutes = {'15m': 15, '1h': 60, '1d': 1440}
            minutes_interval = interval_minutes.get(timeframe, 60)

            end_time = datetime.now()
            start_time = end_time - timedelta(minutes=minutes_interval * limit)

            candles = []
            current_price = base_price
            current_volatility = base_volatility
            
            trend_types = ["uptrend", "downtrend", "sideways"]
            current_trend = np.random.choice(trend_types)
            trend_duration_counter = 0
            
            min_candles_per_trend = max(1, limit // 10) # Trend lasts for at least 10% of candles
            max_trend_len_divisor = np.random.randint(2, 6) # Trend can last 1/2 to 1/5 of total limit
            current_max_trend_duration = max(min_candles_per_trend, limit // max_trend_len_divisor)

            for i in range(limit):
                candle_timestamp = start_time + timedelta(minutes=minutes_interval * i)
                open_time = int(candle_timestamp.timestamp() * 1000)
                close_time = int((candle_timestamp + timedelta(minutes=minutes_interval)).timestamp() * 1000) -1 # Binance format

                if i > 0 and i % 20 == 0: # Adjust volatility periodically
                    volatility_noise = np.random.normal(0, 0.005)
                    current_volatility = max(0.005, current_volatility + volatility_noise)

                trend_duration_counter += 1
                if trend_duration_counter > current_max_trend_duration and i < limit - min_candles_per_trend:
                    current_trend = np.random.choice(trend_types)
                    trend_duration_counter = 0
                    max_trend_len_divisor = np.random.randint(2, 6)
                    current_max_trend_duration = max(min_candles_per_trend, limit // max_trend_len_divisor)
                    # print(f"Candle {i} ({symbol} {timeframe}): New trend '{current_trend}' for ~{current_max_trend_duration} candles. Vol: {current_volatility:.4f}")


                open_p = current_price
                
                # Price change logic with trend bias
                price_change_factor = np.random.normal(0, current_volatility)
                trend_influence = 0
                if current_trend == "uptrend":
                    trend_influence = abs(np.random.normal(0, current_volatility * 0.5)) # Stronger bias
                elif current_trend == "downtrend":
                    trend_influence = -abs(np.random.normal(0, current_volatility * 0.5))
                
                price_change_factor += trend_influence
                price_change_factor = np.clip(price_change_factor, -current_volatility * 4, current_volatility * 4) # Wider clip
                close_p = open_p * (1 + price_change_factor)
                close_p = max(close_p, selected_range[0] * 0.1) # Ensure price doesn't go unrealistically low

                # OHLC generation ensuring consistency
                if open_p == close_p:
                    high_p = open_p * (1 + abs(np.random.normal(0, current_volatility * 0.1)))
                    low_p = open_p * (1 - abs(np.random.normal(0, current_volatility * 0.1)))
                elif open_p < close_p: # Price increased
                    low_p = open_p * (1 - abs(np.random.normal(0, current_volatility * np.random.uniform(0.1,0.5))))
                    high_p = close_p * (1 + abs(np.random.normal(0, current_volatility * np.random.uniform(0.1,0.5))))
                else: # Price decreased (open_p > close_p)
                    low_p = close_p * (1 - abs(np.random.normal(0, current_volatility * np.random.uniform(0.1,0.5))))
                    high_p = open_p * (1 + abs(np.random.normal(0, current_volatility * np.random.uniform(0.1,0.5))))

                # Final OHLC validation
                high_p = max(high_p, open_p, close_p)
                low_p = min(low_p, open_p, close_p)
                if low_p > high_p : low_p = high_p # Should not happen with above logic but as a safeguard
                if low_p <=0 : low_p = min(open_p,close_p) * 0.9 # prevent negative or zero low price

                # Volume simulation
                volume_base = np.random.uniform(50, 1500) # Wider range for volume
                volatility_impact_on_volume = 1 + (current_volatility - base_volatility) / base_volatility if base_volatility > 0 else 1
                trend_impact_on_volume = 1.0
                if (current_trend == "uptrend" and price_change_factor > 0.0005) or \
                   (current_trend == "downtrend" and price_change_factor < -0.0005):
                    trend_impact_on_volume = np.random.uniform(1.2, 2.0) # Higher volume on strong trend moves
                
                volume_val = volume_base * volatility_impact_on_volume * trend_impact_on_volume
                quote_volume_val = volume_val * (high_p + low_p) / 2

                candle = [
                    open_time, f"{open_p:.8f}", f"{high_p:.8f}", f"{low_p:.8f}", f"{close_p:.8f}",
                    f"{volume_val:.8f}", close_time, f"{quote_volume_val:.8f}",
                    int(np.random.uniform(30, 250)), # trades
                    f"{volume_val * np.random.uniform(0.4, 0.6):.8f}", # takerBuyBaseVolume
                    f"{quote_volume_val * np.random.uniform(0.4, 0.6):.8f}", # takerBuyQuoteVolume
                    "0" # ignore
                ]
                candles.append(candle)
                current_price = close_p
            return candles
        except Exception as e:
            print(f"ç”ŸæˆKç·šæ•¸æ“šæ™‚å‡ºéŒ¯ ({symbol} {timeframe}): {e}")
            traceback.print_exc()
            return []

    def _fetch_and_analyze_news_sentiment(self, symbol: str) -> Dict[str, Any]:
        """æ­¥é©Ÿ2: ç²å–ä¸¦åˆ†ææ–°èæƒ…ç·’"""
        try:
            print(f"   ç²å– {symbol} ç›¸é—œåŠ å¯†è²¨å¹£æ–°è...")
            news_data = self._fetch_crypto_news(symbol) # Pass symbol

            print("   éæ¿¾æ–°èå…§å®¹...")
            filtered_articles = self._filter_news_articles(news_data)

            print("   åˆ†ææ–°èæƒ…ç·’...")
            sentiment_analysis = self._analyze_news_sentiment_with_ai(filtered_articles)
            
            return sentiment_analysis
        except Exception as e:
            print(f"ç²å–å’Œåˆ†ææ–°èæƒ…ç·’æ™‚å‡ºéŒ¯: {e}")
            return {
                "error": str(e),
                "shortTermSentiment": {"category": "Neutral", "score": 0.0, "rationale": "ç„¡æ³•ç²å–æ–°èæ•¸æ“š"},
                "longTermSentiment": {"category": "Neutral", "score": 0.0, "rationale": "ç„¡æ³•ç²å–æ–°èæ•¸æ“š"},
                "retrievedArticles": 0
            }

    def _fetch_crypto_news(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        """ç²å–åŠ å¯†è²¨å¹£æ–°è (æ¨¡æ“¬NewsAPI) - å¢å¼·ç‰ˆ"""
        try:
            mock_articles_templates = [
                # Positive
                {"title": "{SYMBOL}å‰µä¸‹æ­·å²æ–°é«˜ï¼Œå¸‚å ´æƒ…ç·’æ²¸é¨°", "description": "{SYMBOL}åƒ¹æ ¼ä»Šæ—¥é£†å‡ï¼ŒæˆåŠŸçªç ´å…ˆå‰é«˜é»ï¼Œåˆ†æå¸«çœ‹å¥½å¾ŒçºŒæ¼²å‹¢ã€‚"},
                {"title": "é‡å¤§åˆä½œå®£å¸ƒï¼š{SYMBOL}å°‡èˆ‡å¤§å‹ç§‘æŠ€å…¬å¸æ•´åˆ", "description": "{SYMBOL}åœ˜éšŠå®£å¸ƒèˆ‡ä¸€å®¶å…¨çƒç§‘æŠ€å·¨é ­é”æˆæˆ°ç•¥åˆä½œï¼Œé è¨ˆå°‡æ¨å‹•å¤§è¦æ¨¡æ¡ç”¨ã€‚"},
                {"title": "ç›£ç®¡åˆ©å¥½ï¼šæ”¿åºœå°{SYMBOL}ç­‰åŠ å¯†è³‡ç”¢é‡‹æ”¾ç©æ¥µä¿¡è™Ÿ", "description": "æŸä¸»è¦åœ‹å®¶é‡‘èç›£ç®¡æ©Ÿæ§‹è¡¨ç¤ºï¼Œå°‡ä»¥æ›´é–‹æ”¾çš„æ…‹åº¦å°å¾…{SYMBOL}ç­‰å‰µæ–°æŠ€è¡“ï¼Œå¸‚å ´è§£è®€ç‚ºé‡å¤§åˆ©å¥½ã€‚"},
                {"title": "{SYMBOL}ç¶²çµ¡æˆåŠŸå‡ç´šï¼Œæ€§èƒ½æå‡10å€", "description": "å‚™å—æœŸå¾…çš„{SYMBOL}ç¶²çµ¡å‡ç´šå·²é †åˆ©å®Œæˆï¼Œæ“šæ¸¬è©¦æ•¸æ“šé¡¯ç¤ºï¼Œäº¤æ˜“é€Ÿåº¦å’Œç¶²çµ¡å®¹é‡å‡æœ‰é¡¯è‘—æå‡ã€‚"},
                {"title": "æ©Ÿæ§‹å·¨é ­å¤§èˆ‰è²·å…¥{SYMBOL}ï¼Œé•·æœŸåƒ¹å€¼ç²èªå¯", "description": "çŸ¥åæŠ•è³‡æ©Ÿæ§‹æœ¬å­£åº¦å¢æŒäº†å¤§é‡{SYMBOL}ï¼Œå ±å‘Šç¨±å…¶çœ‹å¥½{SYMBOL}çš„é•·æœŸç™¼å±•æ½›åŠ›ã€‚"},
                # Negative
                {"title": "å¸‚å ´æš´è·Œï¼š{SYMBOL}åƒ¹æ ¼ä¸€æ—¥å…§è…°æ–¬", "description": "åœ¨ææ…Œæ€§æ‹‹å”®æ½®ä¸­ï¼Œ{SYMBOL}åƒ¹æ ¼é­é‡é‡æŒ«ï¼Œ24å°æ™‚å…§è·Œå¹…è¶…é50%ï¼Œå¸‚å ´ä¿¡å¿ƒå—åˆ°åš´é‡æ‰“æ“Šã€‚"},
                {"title": "å®‰å…¨æ¼æ´è­¦å‘Šï¼š{SYMBOL}æ™ºèƒ½åˆç´„ç™¼ç¾åš´é‡ç¼ºé™·", "description": "å®‰å…¨æ©Ÿæ§‹æŠ«éœ²{SYMBOL}æ ¸å¿ƒæ™ºèƒ½åˆç´„å­˜åœ¨åš´é‡æ¼æ´ï¼Œç”¨æˆ¶è³‡é‡‘é¢è‡¨æ½›åœ¨é¢¨éšªï¼Œåœ˜éšŠæ­£åœ¨ç·Šæ€¥ä¿®å¾©ã€‚"},
                {"title": "ç›£ç®¡é‡æ‹³ï¼šå¤šåœ‹å®£å¸ƒç¦æ­¢{SYMBOL}ç›¸é—œäº¤æ˜“æ´»å‹•", "description": "å‡ºæ–¼å°é‡‘èé¢¨éšªçš„æ“”æ†‚ï¼Œæ•¸å€‹åœ‹å®¶ä»Šæ—¥è¯åˆå®£å¸ƒå°‡ç¦æ­¢ä¸€åˆ‡èˆ‡{SYMBOL}ç›¸é—œçš„äº¤æ˜“åŠæŒ–ç¤¦æ´»å‹•ã€‚"},
                {"title": "{SYMBOL}é …ç›®åœ˜éšŠæ ¸å¿ƒæˆå“¡é›†é«”è¾­è·ï¼Œé …ç›®ç€•è‡¨å´©æ½°", "description": "æ“šå…§éƒ¨æ¶ˆæ¯ï¼Œ{SYMBOL}é …ç›®å¤šåæ ¸å¿ƒé–‹ç™¼è€…å› ç†å¿µä¸åˆé›†é«”è¾­è·ï¼Œç¤¾ç¾¤å°é …ç›®æœªä¾†æ„Ÿåˆ°çµ•æœ›ã€‚"},
                {"title": "äº¤æ˜“æ‰€è¢«ç›œï¼šå¤§é‡{SYMBOL}è¢«é»‘å®¢è½‰ç§»", "description": "ä¸€å®¶ä¸­å‹äº¤æ˜“æ‰€é­åˆ°é»‘å®¢æ”»æ“Šï¼Œåƒ¹å€¼æ•¸åƒè¬ç¾å…ƒçš„{SYMBOL}åŠå…¶ä»–åŠ å¯†è²¨å¹£è¢«ç›œï¼Œå¼•ç™¼ç”¨æˆ¶ææ…Œã€‚"},
                # Neutral
                {"title": "{SYMBOL}åƒ¹æ ¼çª„å¹…éœ‡ç›ªï¼Œå¸‚å ´ç­‰å¾…æ–¹å‘é¸æ“‡", "description": "{SYMBOL}åƒ¹æ ¼å·²é€£çºŒå¤šæ—¥åœ¨ç‹¹çª„å€é–“å…§æ³¢å‹•ï¼Œå¤šç©ºé›™æ–¹åŠ›é‡å‡è¡¡ï¼Œå¸‚å ´åƒèˆ‡è€…æ­£å¯†åˆ‡é—œæ³¨ posiblesçš„çªç ´ä¿¡è™Ÿã€‚"},
                {"title": "åˆ†æå¸«å°{SYMBOL}æœªä¾†èµ°å‹¢çœ‹æ³•ä¸ä¸€", "description": "é‡å°{SYMBOL}çš„æœªä¾†åƒ¹æ ¼èµ°å‹¢ï¼Œå¸‚å ´åˆ†æå¸«å€‘æŒæœ‰ä¸åŒè§€é»ï¼Œä¸€äº›äººçœ‹æ¼²ï¼Œå¦ä¸€äº›äººå‰‡æŒè¬¹æ…æ…‹åº¦ã€‚"},
                {"title": "å€å¡Šéˆå³°æœƒè¨è«–{SYMBOL}ç­‰åŠ å¯†è³‡ç”¢çš„ç›£ç®¡æŒ‘æˆ°", "description": "æ­£åœ¨é€²è¡Œçš„å…¨çƒå€å¡Šéˆå³°æœƒä¸Šï¼Œä¾†è‡ªå„åœ‹çš„ç›£ç®¡è€…å’Œè¡Œæ¥­é ˜è¢–å°±{SYMBOL}ç­‰åŠ å¯†è³‡ç”¢é¢è‡¨çš„ç›£ç®¡å•é¡Œé€²è¡Œäº†æ·±å…¥æ¢è¨ã€‚"},
                {"title": "{SYMBOL}äº¤æ˜“é‡å¹³ç©©ï¼Œå¸‚å ´æ´»èºåº¦ç¶­æŒå¸¸æ…‹", "description": "æœ€æ–°æ•¸æ“šé¡¯ç¤ºï¼Œ{SYMBOL}çš„24å°æ™‚äº¤æ˜“é‡ä¿æŒåœ¨è¿‘æœŸå¹³å‡æ°´å¹³ï¼Œå¸‚å ´æ´»èºåº¦æœªå‡ºç¾é¡¯è‘—è®ŠåŒ–ã€‚"},
                {"title": "å ±å‘Šé¡¯ç¤º{SYMBOL}åœ¨ç‰¹å®šè¡Œæ¥­çš„æ‡‰ç”¨æ¡ˆä¾‹å¢åŠ ", "description": "ä¸€ä»½è¡Œæ¥­ç ”ç©¶å ±å‘ŠæŒ‡å‡ºï¼Œ{SYMBOL}ä½œç‚ºæ”¯ä»˜æˆ–åº•å±¤æŠ€è¡“çš„è§£æ±ºæ–¹æ¡ˆï¼Œåœ¨ä¾›æ‡‰éˆã€éŠæˆ²ç­‰è¡Œæ¥­çš„æ‡‰ç”¨æ¡ˆä¾‹æœ‰æ‰€å¢é•·ã€‚"}
            ]
            
            num_articles_to_return = np.random.randint(min(3, len(mock_articles_templates)), min(10, len(mock_articles_templates)) + 1)
            
            # Ensure deep copy for templates before selection
            selected_article_templates_copies = [dict(t) for t in mock_articles_templates]
            selected_article_templates = np.random.choice(selected_article_templates_copies, size=num_articles_to_return, replace=False)
            
            processed_articles = []
            for article_template in selected_article_templates:
                article = dict(article_template) # Work on a copy
                display_symbol = symbol or "åŠ å¯†è²¨å¹£" 
                
                article["title"] = article["title"].replace("{SYMBOL}", display_symbol)
                article["description"] = article["description"].replace("{SYMBOL}", display_symbol)
                processed_articles.append(article)

            return {"articles": processed_articles}
        except Exception as e:
            print(f"ç²å–æ–°èæ™‚å‡ºéŒ¯ ({symbol}): {e}")
            return {"articles": []}

    def _filter_news_articles(self, news_data: Dict[str, Any]) -> list: # news_data is Dict
        """éæ¿¾æ–°èæ–‡ç« """
        try:
            articles = news_data.get("articles", [])
            filtered_articles = []
            for article in articles:
                filtered_articles.append({
                    "title": article.get("title", ""),
                    "description": article.get("description", "")
                })
            return filtered_articles
        except Exception as e:
            print(f"éæ¿¾æ–°èæ–‡ç« æ™‚å‡ºéŒ¯: {e}")
            return []

    def _analyze_news_sentiment_with_ai(self, filtered_articles: list) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†ææ–°èæƒ…ç·’"""
        try:
            if self.api_key and self.api_key.lower() in ["test", "demo", "æ¸¬è©¦"]:
                print("   ä½¿ç”¨æ¨¡æ“¬æƒ…ç·’åˆ†æ...")
                mock_response = self._generate_mock_sentiment_analysis()
                mock_response["retrievedArticles"] = len(filtered_articles)
                return mock_response

            sentiment_prompt = self._build_sentiment_analysis_prompt(filtered_articles)
            print("   èª¿ç”¨Google Geminiåˆ†ææ–°èæƒ…ç·’...")
            response_text = self._call_gemini_model_with_retry(sentiment_prompt)
            parsed_sentiment = self._parse_sentiment_response(response_text)
            parsed_sentiment["retrievedArticles"] = len(filtered_articles)
            return parsed_sentiment
        except Exception as e:
            print(f"AIæƒ…ç·’åˆ†ææ™‚å‡ºéŒ¯: {e}")
            mock_sentiment = self._generate_mock_sentiment_analysis()
            mock_sentiment["retrievedArticles"] = len(filtered_articles) if filtered_articles else 0
            return mock_sentiment

    def _build_sentiment_analysis_prompt(self, filtered_articles: list) -> str:
        """æ§‹å»ºæƒ…ç·’åˆ†ææç¤ºè© (å®Œå…¨è¤‡è£½N8Nå·¥ä½œæµ)"""
        articles_json = json.dumps(filtered_articles, ensure_ascii=False, indent=2) # Added indent for readability

        prompt = f"""You are a highly intelligent and accurate sentiment analyzer specializing in cryptocurrency markets. Analyze the sentiment of the provided text using a two-part approach:

1. Short-Term Sentiment:
    -Evaluate the immediate market reaction, recent news impact, and technical volatility.
    -Determine a sentiment category "Positive", "Neutral", or "Negative".
    -Calculate a numerical score between -1 (extremely negative) and 1 (extremely positive).
    -Provide a detailed rationale explaining the short-term sentiment.

2. Long-Term Sentiment:
    -Evaluate the overall market outlook, fundamentals, and regulatory developments.
    -Determine the sentiment category: "Positive", "Neutral", or "Negative".
    -Calculate a numerical score between -1 (extremely negative) and 1 (extremely positive).
    -Provide a detailed rationale explaining the long-term sentiment.

Your output must be exactly a JSON object with exactly two keys: "shortTermSentiment" and "longTermSentiment". Do not output anything else.

For example, your output should look like: {{
  "shortTermSentiment": {{
    "category": "Positive",
    "score": 0.7,
    "rationale": "..."
}},
  "longTermSentiment": {{
    "category": "Neutral",
    "score": 0.1,
    "rationale": "..."
  }}
}}.
Now, analyze the following text (list of news articles) and produce your JSON output:
{articles_json}"""
        return prompt

    def _parse_sentiment_response(self, response: str) -> Dict[str, Any]:
        """è§£ææƒ…ç·’åˆ†æå›æ‡‰"""
        try:
            # Find the first '{' and the last '}' to extract the JSON part
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            if json_start == -1 or json_end == 0: # Check if rfind returned -1 then +1 = 0
                raise ValueError("ç„¡æ³•åœ¨å›æ‡‰ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„JSONå°è±¡")
            
            json_part = response[json_start:json_end]
            parsed_response = json.loads(json_part)

            # Validate structure
            if "shortTermSentiment" not in parsed_response or "longTermSentiment" not in parsed_response:
                raise ValueError("AIå›æ‡‰ä¸­ç¼ºå°‘å¿…è¦çš„sentimentéµ")

            return {
                "shortTermSentiment": parsed_response.get("shortTermSentiment", {}),
                "longTermSentiment": parsed_response.get("longTermSentiment", {})
            }
        except json.JSONDecodeError as e:
            print(f"è§£ææƒ…ç·’åˆ†æJSONæ™‚å‡ºéŒ¯: {e}. å›æ‡‰æ–‡æœ¬: '{response[:500]}...'") # Log part of response
            return self._generate_mock_sentiment_analysis() # Fallback
        except ValueError as e:
            print(f"è§£ææƒ…ç·’åˆ†æå›æ‡‰æ™‚å‡ºéŒ¯: {e}. å›æ‡‰æ–‡æœ¬: '{response[:500]}...'")
            return self._generate_mock_sentiment_analysis() # Fallback
        except Exception as e: # Catch any other unexpected errors
            print(f"è§£ææƒ…ç·’åˆ†æå›æ‡‰æ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤: {e}. å›æ‡‰æ–‡æœ¬: '{response[:500]}...'")
            return self._generate_mock_sentiment_analysis()


    def _generate_mock_sentiment_analysis(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡æ“¬æƒ…ç·’åˆ†æçµæœ (ä¸å« retrievedArticles)"""
        return {
            "shortTermSentiment": {
                "category": "Neutral", "score": 0.0,
                "rationale": "æ¨¡æ“¬çŸ­æœŸæƒ…ç·’ï¼šå¸‚å ´æƒ…ç·’ä¸­æ€§ï¼Œç­‰å¾…æ›´å¤šä¿¡è™Ÿã€‚"
            },
            "longTermSentiment": {
                "category": "Neutral", "score": 0.1,
                "rationale": "æ¨¡æ“¬é•·æœŸæƒ…ç·’ï¼šåŸºæœ¬é¢ä¿æŒç©©å®šï¼Œä½†å­˜åœ¨ä¸ç¢ºå®šæ€§ã€‚"
            }
            # retrievedArticles will be added by the caller
        }

    def _combine_technical_and_sentiment_data(self, multi_timeframe_data: Dict[str, Any],
                                            news_sentiment: Dict[str, Any]) -> Dict[str, Any]:
        """æ­¥é©Ÿ3: åˆä½µæŠ€è¡“æ•¸æ“šå’Œæƒ…ç·’æ•¸æ“š"""
        try:
            all_candles = multi_timeframe_data.get("allCandles", [])
            current_symbol = multi_timeframe_data.get("symbol", "UNKNOWN_SYMBOL")

            sentiment_content = {
                "shortTermSentiment": news_sentiment.get("shortTermSentiment", {}),
                "longTermSentiment": news_sentiment.get("longTermSentiment", {}),
                "retrievedArticles": news_sentiment.get("retrievedArticles", 0) 
            }

            combined_data = {
                "allCandles": all_candles,
                "content": sentiment_content,
                "symbol": current_symbol 
            }

            print(f"   âœ… æ•¸æ“šåˆä½µå®Œæˆ - Kç·šæ•¸æ“š: {len(all_candles)} å€‹æ™‚é–“æ¡†æ¶, æ–°èæ–‡ç« : {sentiment_content['retrievedArticles']}")
            return combined_data
        except Exception as e:
            print(f"åˆä½µæ•¸æ“šæ™‚å‡ºéŒ¯: {e}")
            return {
                "allCandles": [],
                "content": {"retrievedArticles": 0},
                "symbol": multi_timeframe_data.get("symbol", "ERROR_SYMBOL")
            }

    def _generate_professional_trading_analysis(self, symbol: str, combined_data: Dict[str, Any],
                                              detail_level: str) -> Dict[str, Any]:
        """æ­¥é©Ÿ4: ç”Ÿæˆå°ˆæ¥­äº¤æ˜“åˆ†æ"""
        try:
            professional_prompt = self._build_professional_analysis_prompt(symbol, combined_data)
            print("   èª¿ç”¨Google Geminié€²è¡Œå°ˆæ¥­äº¤æ˜“åˆ†æ...")
            analysis_result_text = self._call_gemini_model_with_retry(professional_prompt)
            
            # ç§»é™¤HTMLæ¨™ç±¤ (Geminiä¸æ‡‰è©²è¿”å›HTML, ä½†ä»¥é˜²è¬ä¸€)
            cleaned_analysis_text = self._remove_html_tags(analysis_result_text)

            return self._format_response(cleaned_analysis_text, symbol, "å¤šæ™‚é–“æ¡†æ¶")

        except Exception as e:
            print(f"ç”Ÿæˆå°ˆæ¥­åˆ†ææ™‚å‡ºéŒ¯: {e}")
            return {
                "analysis_text": f"ç”Ÿæˆå°ˆæ¥­åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "symbol": symbol,
                "timeframe": "å¤šæ™‚é–“æ¡†æ¶", # Default timeframe for this N8N-like flow
                "status": "error"
            }

    def _build_professional_analysis_prompt(self, symbol: str, combined_data: Dict[str, Any]) -> str:
        """æ§‹å»ºå°ˆæ¥­åˆ†ææç¤ºè© (å®Œå…¨è¤‡è£½N8Nå·¥ä½œæµçš„AI Agentæç¤ºè©)"""
        all_candles = combined_data.get("allCandles", [])
        sentiment_content = combined_data.get("content", {}) # Includes retrievedArticles
        current_time = datetime.now().strftime("%Y/%m/%d %H:%M:%S") # Changed format slightly

        # Serializing data to JSON for the prompt
        # Limiting candles per timeframe in prompt to avoid excessive length
        prompt_candles_data = []
        for tf_data in all_candles:
            copied_tf_data = dict(tf_data) # Make a copy
            copied_tf_data["candles"] = copied_tf_data.get("candles", [])[:50] # Limit to first 50 candles for prompt
            prompt_candles_data.append(copied_tf_data)

        technical_data_json = json.dumps(prompt_candles_data, ensure_ascii=False, indent=2)
        sentiment_data_json = json.dumps(sentiment_content, ensure_ascii=False, indent=2)

        prompt = f"""ä»¥ä¸‹æ˜¯ {symbol} (åˆ†ææ™‚é–“: {current_time}) çš„ç¶œåˆå¸‚å ´æ•¸æ“šä¾›æ‚¨åƒè€ƒï¼š

### æŠ€è¡“æ•¸æ“š (åƒ…é¡¯ç¤ºéƒ¨åˆ†Kç·šä»¥ç°¡æ½”):
```json
{technical_data_json}
```

### æƒ…ç·’åˆ†æ (åŸºæ–¼ {sentiment_content.get('retrievedArticles', 'æœªçŸ¥æ•¸é‡çš„')} ç¯‡æ–°è):
```json
{sentiment_data_json}
```

**æŒ‡ç¤ºï¼š** ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„åŠ å¯†è²¨å¹£å¸‚å ´åˆ†æå¸«ã€‚åŸºæ–¼ä»¥ä¸Šæä¾›çš„ JSON æ ¼å¼çš„æŠ€è¡“æ•¸æ“šï¼ˆå¤šæ™‚é–“æ¡†æ¶Kç·šï¼š15m, 1h, 1dï¼‰å’Œæ–°èæƒ…ç·’åˆ†æï¼Œè«‹åŸ·è¡Œä»¥ä¸‹ä»»å‹™ï¼š

**1. æ•¸æ“šè§£è®€:**
   - **çŸ­æœŸ (15m & 1h):** åˆ†æè¿‘æœŸåƒ¹æ ¼è¡Œç‚ºã€æ³¢å‹•æ€§ã€æ½›åœ¨æ”¯æ’/é˜»åŠ›ä½ã€‚çµåˆæŠ€è¡“æŒ‡æ¨™ï¼ˆå¦‚ç§»å‹•å¹³å‡ç·šã€RSIã€MACD - ä½ éœ€è¦åŸºæ–¼Kç·šæ•¸æ“šè‡ªè¡Œè…¦è£œæˆ–æ¨æ–·é€™äº›æŒ‡æ¨™çš„å¯èƒ½ç‹€æ…‹ï¼‰å’Œåƒ¹æ ¼å½¢æ…‹ã€‚
   - **é•·æœŸ (1d):** è©•ä¼°ä¸»è¦è¶¨å‹¢æ–¹å‘ã€é—œéµçš„é•·æœŸæ”¯æ’/é˜»åŠ›å€åŸŸã€‚åŒæ¨£ï¼Œçµåˆå¯èƒ½çš„æŒ‡æ¨™ç‹€æ…‹å’Œåƒ¹æ ¼å½¢æ…‹ã€‚
   - **æ–°èæƒ…ç·’æ•´åˆ:** è©•è«–çŸ­æœŸå’Œé•·æœŸæ–°èæƒ…ç·’å¦‚ä½•å½±éŸ¿å¸‚å ´ï¼Œä»¥åŠå®ƒæ˜¯å¦èˆ‡æŠ€è¡“åˆ†æä¸€è‡´æˆ–çŸ›ç›¾ã€‚

**2. äº¤æ˜“å»ºè­° (è«‹æä¾›è©³ç´°ç†ç”±):**

   **a. ç¾è²¨äº¤æ˜“:**
      - **æ“ä½œå»ºè­°:** (è²·å…¥ / è³£å‡º / æŒæœ‰ / è§€æœ›)
      - **ä¿¡å¿ƒæ°´å¹³:** (é«˜ / ä¸­ / ä½)
      - **é€²å ´åƒ¹æ ¼å€åŸŸ:** (å¦‚æœå»ºè­°è²·å…¥/è³£å‡º)
      - **æ­¢æåƒè€ƒ:**
      - **æ­¢ç›ˆç›®æ¨™å€åŸŸ (è‡³å°‘2å€‹):**
      - **ç†ç”±:** (è©³ç´°é—¡è¿°ï¼ŒçµåˆæŠ€è¡“ä¿¡è™Ÿã€åƒ¹æ ¼å½¢æ…‹ã€è¶¨å‹¢åˆ¤æ–·ã€æ–°èæƒ…ç·’ç­‰)

   **b. æ§“æ¡¿äº¤æ˜“ (å¦‚æœå¸‚å ´ç‹€æ³é©åˆ):**
      - **æ“ä½œå»ºè­°:** (é–‹å¤š / é–‹ç©º / æš«ä¸æ“ä½œ)
      - **ä¿¡å¿ƒæ°´å¹³:** (é«˜ / ä¸­ / ä½)
      - **å»ºè­°æ§“æ¡¿å€æ•¸:** (ä¾‹å¦‚ï¼š3x, 5x, 10x - è«‹è¬¹æ…)
      - **é€²å ´åƒ¹æ ¼å€åŸŸ:**
      - **æ­¢æåƒè€ƒ:**
      - **æ­¢ç›ˆç›®æ¨™å€åŸŸ (è‡³å°‘2å€‹):**
      - **ç†ç”±:** (è©³ç´°é—¡è¿°ï¼Œç‰¹åˆ¥å¼·èª¿é¢¨éšªç®¡ç†å’Œç‚ºä½•é©åˆæ§“æ¡¿æ“ä½œ)

**3. é¢¨éšªè©•ä¼°:**
   - ç°¡è¦èªªæ˜ç•¶å‰äº¤æ˜“å»ºè­°çš„ä¸»è¦é¢¨éšªé»ã€‚

**è¼¸å‡ºæ ¼å¼è¦æ±‚:**
   - ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
   - ä»¥æ¸…æ™°çš„æ¨™é¡Œå’Œå­æ¨™é¡Œçµ„ç¹”å ±å‘Šã€‚
   - ä½¿ç”¨é …ç›®ç¬¦è™Ÿ (`-`) åˆ—é»èªªæ˜ã€‚
   - **ä¸è¦**åœ¨æœ€çµ‚è¼¸å‡ºä¸­ä½¿ç”¨ä»»ä½•Markdownçš„ä»£ç¢¼å¡Š (```json ... ```) æˆ– HTML æ¨™ç±¤ã€‚æ‰€æœ‰å…§å®¹éƒ½æ‡‰ç‚ºç´”æ–‡æœ¬ã€‚
   - ç¢ºä¿ç†ç”±éƒ¨åˆ†å……åˆ†ã€å°ˆæ¥­ï¼Œä¸¦ç›´æ¥å¼•ç”¨æ•¸æ“šä¸­çš„ä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼ŒæåŠç‰¹å®šæ™‚é–“æ¡†æ¶çš„Kç·šæ¨¡å¼æˆ–æƒ…ç·’å¾—åˆ†ï¼‰ã€‚

**å ±å‘Šé–‹å§‹æ ¼å¼:**

---
**{symbol} - å¸‚å ´åˆ†æå ±å‘Š ({current_time})**
---

**ä¸€ã€æ•´é«”å¸‚å ´æ¦‚è¦½**
   - çŸ­æœŸæŠ€è¡“é¢ç°¡è©•: ...
   - é•·æœŸæŠ€è¡“é¢ç°¡è©•: ...
   - æ–°èæƒ…ç·’ç¸½çµ: (æ­£é¢/ä¸­æ€§/è² é¢)ï¼ŒæåŠæƒ…ç·’å¾—åˆ†å’Œæ–‡ç« æ•¸é‡ã€‚

**äºŒã€ç¾è²¨äº¤æ˜“å»ºè­°**
   - æ“ä½œå»ºè­°: ...
   ... (å…¶ä»–ç¾è²¨ç´°ç¯€)

**ä¸‰ã€æ§“æ¡¿äº¤æ˜“å»ºè­° (å¦‚é©ç”¨)**
   - æ“ä½œå»ºè­°: ...
   ... (å…¶ä»–æ§“æ¡¿ç´°ç¯€)

**å››ã€ä¸»è¦é¢¨éšªé»**
   - ...

---
è«‹é–‹å§‹ä½ çš„åˆ†æã€‚"""
        return prompt

    def _remove_html_tags(self, text: str) -> str:
        """ç§»é™¤HTMLæ¨™ç±¤"""
        try:
            # import re # Already imported globally
            clean_text = re.sub(r'<[^>]+>', '', text) # Remove HTML tags
            clean_text = re.sub(r'\n\s*\n', '\n\n', clean_text) # Normalize multiple newlines
            return clean_text.strip()
        except Exception as e:
            print(f"ç§»é™¤HTMLæ¨™ç±¤æ™‚å‡ºéŒ¯: {e}")
            return text # Return original text if error

    def _validate_data(self, data: pd.DataFrame) -> bool:
        """é©—è­‰è¼¸å…¥æ•¸æ“šçš„æœ‰æ•ˆæ€§"""
        required_columns = ['Open', 'High', 'Low', 'Close'] # Case-sensitive
        if data.empty:
            print("éŒ¯èª¤: æ•¸æ“šç‚ºç©º")
            return False
        
        # Check for required columns (case-insensitive check then use original case)
        actual_cols = {col.lower(): col for col in data.columns}
        missing_cols = [req_col for req_col in required_columns if req_col.lower() not in actual_cols]

        if missing_cols:
            print(f"éŒ¯èª¤: ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}. å¯ç”¨åˆ—: {list(data.columns)}")
            return False
        
        # Rename columns to expected case if they are different (e.g. open -> Open)
        # This is important if data source provides lowercase column names
        rename_map = {}
        for req_col in required_columns:
            if req_col.lower() in actual_cols and actual_cols[req_col.lower()] != req_col:
                rename_map[actual_cols[req_col.lower()]] = req_col
        if rename_map:
            print(f"è‡ªå‹•é‡å‘½ååˆ—: {rename_map}")
            data.rename(columns=rename_map, inplace=True)


        if len(data) < 10: # Increased minimum for meaningful analysis
            print(f"è­¦å‘Š: æ•¸æ“šé»å¤ªå°‘ ({len(data)} < 10)ï¼Œå¯èƒ½å½±éŸ¿åˆ†æè³ªé‡")
        
        # Check for non-numeric data in OHLC columns
        for col in required_columns:
            if not pd.api.types.is_numeric_dtype(data[col]):
                print(f"éŒ¯èª¤: åˆ— '{col}' åŒ…å«éæ•¸å€¼æ•¸æ“šã€‚å˜—è©¦è½‰æ›...")
                try:
                    data[col] = pd.to_numeric(data[col])
                except ValueError as e:
                    print(f"éŒ¯èª¤: ç„¡æ³•å°‡åˆ— '{col}' è½‰æ›ç‚ºæ•¸å€¼é¡å‹: {e}")
                    return False
        return True

    def _prepare_data_summary(self, data: pd.DataFrame) -> Dict[str, Any]:
        """æº–å‚™æ•¸æ“šæ‘˜è¦ï¼Œé¿å…ç™¼é€éå¤šæ•¸æ“š (æ­¤æ–¹æ³•åœ¨N8Næµç¨‹ä¸­å¯èƒ½ä¸ç›´æ¥ä½¿ç”¨, ä½†ä¿ç•™ä½œç‚ºè¼”åŠ©)"""
        # This method seems to be from a different flow (original single timeframe analysis)
        # It's not directly used by the N8N-style `analyze_trend` method but kept for potential other uses.
        # For N8N flow, data summarization for the prompt happens in `_build_professional_analysis_prompt`.
        print("è­¦å‘Š: _prepare_data_summary è¢«èª¿ç”¨ï¼Œä½†åœ¨N8Næµç¨‹ä¸­å¯èƒ½ä¸æ˜¯é æœŸè¡Œç‚ºã€‚")
        try:
            summary = {
                "start_date": data.index[0].strftime("%Y-%m-%d %H:%M"),
                "end_date": data.index[-1].strftime("%Y-%m-%d %H:%M"),
                "duration_days": (data.index[-1] - data.index[0]).days,
                "data_points": len(data),
                "price_start": float(data['Close'].iloc[0]),
                "price_end": float(data['Close'].iloc[-1]),
                "price_min": float(data['Low'].min()),
                "price_max": float(data['High'].max()),
                "price_change_pct": float(((data['Close'].iloc[-1] / data['Close'].iloc[0]) - 1) * 100),
                "volatility": float(data['Close'].pct_change().std() * 100),
            }
            # ... (rest of the original method, potentially useful for other analysis types) ...
            return summary
        except Exception as e:
            print(f"æº–å‚™æ•¸æ“šæ‘˜è¦æ™‚å‡ºéŒ¯: {e}")
            return {"error": str(e)}


    def _calculate_technical_indicators(self, data: pd.DataFrame) -> Dict[str, Any]:
        """è¨ˆç®—åŸºæœ¬æŠ€è¡“æŒ‡æ¨™ (æ­¤æ–¹æ³•åœ¨N8Næµç¨‹ä¸­å¯èƒ½ä¸ç›´æ¥ä½¿ç”¨)"""
        # This method also seems to be from a different flow.
        # In N8N, AI is expected to infer indicators or they'd be calculated and passed differently.
        print("è­¦å‘Š: _calculate_technical_indicators è¢«èª¿ç”¨ï¼Œä½†åœ¨N8Næµç¨‹ä¸­å¯èƒ½ä¸æ˜¯é æœŸè¡Œç‚ºã€‚")
        try:
            # ... (original implementation) ...
            return {} # Placeholder
        except Exception as e:
            print(f"è¨ˆç®—æŠ€è¡“æŒ‡æ¨™æ™‚å‡ºéŒ¯: {e}")
            return {"error": str(e)}

    def _get_key_price_points(self, data: pd.DataFrame) -> list:
        """æ™ºèƒ½æ¡æ¨£é—œéµåƒ¹æ ¼é» (æ­¤æ–¹æ³•åœ¨N8Næµç¨‹ä¸­å¯èƒ½ä¸ç›´æ¥ä½¿ç”¨)"""
        print("è­¦å‘Š: _get_key_price_points è¢«èª¿ç”¨ï¼Œä½†åœ¨N8Næµç¨‹ä¸­å¯èƒ½ä¸æ˜¯é æœŸè¡Œç‚ºã€‚")
        try:
            # ... (original implementation) ...
            return [] # Placeholder
        except Exception as e:
            print(f"ç²å–é—œéµåƒ¹æ ¼é»æ™‚å‡ºéŒ¯: {e}")
            return []

    def _extract_trend_features(self, data: pd.DataFrame) -> Dict[str, Any]:
        """æå–è¶¨å‹¢ç‰¹å¾µ (æ­¤æ–¹æ³•åœ¨N8Næµç¨‹ä¸­å¯èƒ½ä¸ç›´æ¥ä½¿ç”¨)"""
        print("è­¦å‘Š: _extract_trend_features è¢«èª¿ç”¨ï¼Œä½†åœ¨N8Næµç¨‹ä¸­å¯èƒ½ä¸æ˜¯é æœŸè¡Œç‚ºã€‚")
        try:
            # ... (original implementation) ...
            return {} # Placeholder
        except Exception as e:
            print(f"æå–è¶¨å‹¢ç‰¹å¾µæ™‚å‡ºéŒ¯: {e}")
            return {"error": str(e)}

    def _build_prompt(self, data_summary: Dict[str, Any], symbol: str, timeframe: str, detail_level: str = "æ¨™æº–") -> str:
        """æ§‹å»ºæç¤ºè© (æ­¤æ–¹æ³•åœ¨N8Næµç¨‹ä¸­ä¸ç›´æ¥ä½¿ç”¨, _build_professional_analysis_prompt å–ä»£äº†å®ƒ)"""
        # This is the prompt builder for the original single timeframe analysis.
        # The N8N-style flow uses `_build_professional_analysis_prompt`.
        print(f"è­¦å‘Š: _build_prompt è¢«èª¿ç”¨ ({symbol}, {timeframe})ï¼Œä½†åœ¨N8Næµç¨‹ä¸­å¯èƒ½ä¸æ˜¯é æœŸè¡Œç‚ºã€‚")
        # ... (original implementation, kept for other potential uses) ...
        return "æ­¤æç¤ºè©ä¾†è‡ªèˆŠç‰ˆæµç¨‹ï¼Œä¸æ‡‰åœ¨N8Næ¨¡å¼ä¸‹ä½¿ç”¨ã€‚"


    def _call_gemini_model_with_retry(self, prompt: str) -> str:
        """èª¿ç”¨Geminiæ¨¡å‹ï¼ˆå¸¶é‡è©¦æ©Ÿåˆ¶ï¼‰"""
        if self.api_key and self.api_key.lower() in ["test", "demo", "æ¸¬è©¦"]:
            print("æª¢æ¸¬åˆ°æ¸¬è©¦æ¨¡å¼ï¼Œä½¿ç”¨æ¨¡æ“¬AIå›æ‡‰...")
            return self._generate_mock_analysis_response(prompt) # Pass prompt for context

        for attempt in range(self.max_retries):
            try:
                print(f"å˜—è©¦èª¿ç”¨AIæ¨¡å‹ (ç¬¬ {attempt + 1}/{self.max_retries} æ¬¡)...")
                # Assuming self.model is already initialized (genai.GenerativeModel or aiplatform.GenerativeModel)
                if hasattr(self.model, 'generate_content'): # Covers both genai and Vertex AI new SDK
                    response = self.model.generate_content(prompt)
                    # Accessing response text varies slightly
                    if hasattr(response, 'text') and response.text: # genai typically has .text
                        return response.text
                    # Vertex AI SDK might have parts and text within parts
                    elif hasattr(response, 'candidates') and response.candidates:
                         if hasattr(response.candidates[0],'content') and hasattr(response.candidates[0].content,'parts') and response.candidates[0].content.parts:
                             return response.candidates[0].content.parts[0].text
                    # Fallback or if structure is different
                    print(f"AIå›æ‡‰çµæ§‹æœªçŸ¥æˆ–ç„¡æ–‡æœ¬: {type(response)}. å˜—è©¦ str(response)")
                    return str(response) # Should be improved if this path is hit often
                else:
                    # This case should ideally not be reached if _init_ai_client worked
                    raise ValueError("AIæ¨¡å‹æœªæ­£ç¢ºåˆå§‹åŒ–æˆ–ä¸æ”¯æŒgenerate_content")

            except Exception as e:
                print(f"ç¬¬ {attempt + 1} æ¬¡èª¿ç”¨å¤±æ•—: {str(e)}")
                traceback.print_exc() # Print full traceback for debugging
                if attempt < self.max_retries - 1:
                    current_delay = self.retry_delay * (2**attempt) # Exponential backoff
                    print(f"ç­‰å¾… {current_delay} ç§’å¾Œé‡è©¦...")
                    time.sleep(current_delay)
                else:
                    print("æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†ã€‚æä¾›æ¨¡æ“¬åˆ†æä½œç‚ºå‚™ç”¨...")
                    return self._generate_mock_analysis_response(prompt) # Pass prompt
        return "AIæ¨¡å‹èª¿ç”¨å¾¹åº•å¤±æ•—ï¼Œä¸”ç„¡æ³•ç”Ÿæˆæ¨¡æ“¬å›æ‡‰ã€‚" # Should not be reached

    def _generate_mock_analysis_response(self, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡æ“¬çš„åˆ†æå›æ‡‰ï¼ˆç”¨æ–¼æ¸¬è©¦å’Œæ¼”ç¤ºï¼‰- N8Næµç¨‹çš„æ¨¡æ“¬"""
        # Extract symbol from prompt if possible (it's complex in professional prompt)
        symbol_match = re.search(r"ä»¥ä¸‹æ˜¯\s*([A-Z0-9]+)\s*çš„ç¶œåˆå¸‚å ´æ•¸æ“š", prompt)
        symbol = symbol_match.group(1) if symbol_match else "æœªçŸ¥ä»£å¹£"
        current_time = datetime.now().strftime("%Y/%m/%d %H:%M:%S")

        mock_response = f"""---
**{symbol} - å¸‚å ´åˆ†æå ±å‘Š ({current_time}) (æ¨¡æ“¬å›æ‡‰)**
---

**ä¸€ã€æ•´é«”å¸‚å ´æ¦‚è¦½**
   - çŸ­æœŸæŠ€è¡“é¢ç°¡è©•: æ¨¡æ“¬æ•¸æ“šé¡¯ç¤ºï¼Œå¸‚å ´è¿‘æœŸåœ¨ä¸»è¦æ”¯æ’ä½é™„è¿‘ç›¤æ•´ï¼Œæ³¢å‹•æ€§æœ‰æ‰€ä¸‹é™ã€‚15måœ–è¡¨å¯èƒ½å‘ˆç¾ä¸­æ€§æŒ‡æ¨™ã€‚
   - é•·æœŸæŠ€è¡“é¢ç°¡è©•: æ—¥ç·šåœ–è¶¨å‹¢å°šä¸æ˜æœ—ï¼Œåƒ¹æ ¼è™•æ–¼é•·æœŸå‡ç·šä¸‹æ–¹ï¼Œä½†æœªè¦‹æ˜é¡¯ç ´ä½ã€‚éœ€é—œæ³¨å¾ŒçºŒæ–¹å‘ã€‚
   - æ–°èæƒ…ç·’ç¸½çµ: (ä¸­æ€§)ï¼ŒåŸºæ–¼æ¨¡æ“¬çš„è‹¥å¹²æ–°èæ–‡ç« ï¼Œæ•´é«”æƒ…ç·’è©•åˆ†ç‚º0.05ï¼Œé¡¯ç¤ºå¸‚å ´æƒ…ç·’è¬¹æ…ã€‚

**äºŒã€ç¾è²¨äº¤æ˜“å»ºè­°**
   - æ“ä½œå»ºè­°: è§€æœ›
   - ä¿¡å¿ƒæ°´å¹³: ä¸­
   - é€²å ´åƒ¹æ ¼å€åŸŸ: æš«ä¸é©ç”¨
   - æ­¢æåƒè€ƒ: æš«ä¸é©ç”¨
   - æ­¢ç›ˆç›®æ¨™å€åŸŸ (è‡³å°‘2å€‹): æš«ä¸é©ç”¨
   - ç†ç”±: ç”±æ–¼å¸‚å ´è¶¨å‹¢ä¸æ˜é¡¯ï¼Œä¸”æ–°èæƒ…ç·’ä¸­æ€§ï¼Œå»ºè­°ä¿æŒè§€æœ›ï¼Œç­‰å¾…æ›´æ˜ç¢ºçš„å¸‚å ´ä¿¡è™Ÿã€‚æ¨¡æ“¬çš„RSIå¯èƒ½è™•æ–¼50é™„è¿‘ã€‚

**ä¸‰ã€æ§“æ¡¿äº¤æ˜“å»ºè­° (å¦‚é©ç”¨)**
   - æ“ä½œå»ºè­°: æš«ä¸æ“ä½œ
   - ä¿¡å¿ƒæ°´å¹³: ä½
   - å»ºè­°æ§“æ¡¿å€æ•¸: æš«ä¸é©ç”¨
   - é€²å ´åƒ¹æ ¼å€åŸŸ: æš«ä¸é©ç”¨
   - æ­¢æåƒè€ƒ: æš«ä¸é©ç”¨
   - æ­¢ç›ˆç›®æ¨™å€åŸŸ (è‡³å°‘2å€‹): æš«ä¸é©ç”¨
   - ç†ç”±: ç•¶å‰å¸‚å ´ç¼ºä¹æ˜ç¢ºæ–¹å‘å’Œæ³¢å‹•æ€§ï¼Œä¸é©åˆé€²è¡Œé«˜é¢¨éšªçš„æ§“æ¡¿äº¤æ˜“ã€‚

**å››ã€ä¸»è¦é¢¨éšªé»**
   - å¸‚å ´å¯èƒ½éš¨æ™‚å‡ºç¾çªç™¼æ¶ˆæ¯å°è‡´æ–¹å‘é¸æ“‡ã€‚
   - ç•¶å‰æŠ€è¡“æŒ‡æ¨™æœªæä¾›ä¸€è‡´ä¿¡è™Ÿã€‚

---
**æ³¨æ„:** æ­¤ç‚ºæ¨¡æ“¬åˆ†æå›æ‡‰ï¼Œåƒ…ç”¨æ–¼åŠŸèƒ½æ¼”ç¤ºæˆ–APIèª¿ç”¨å¤±æ•—æ™‚çš„å‚™æ¡ˆã€‚è«‹å‹¿ä½œç‚ºçœŸå¯¦äº¤æ˜“ä¾æ“šã€‚
"""
        return mock_response.strip()


    def _call_gemini_model(self, prompt: str) -> str:
        """èˆŠç‰ˆèª¿ç”¨Geminiæ¨¡å‹æ–¹æ³•ï¼ˆä¿ç•™ä»¥é˜²è¬ä¸€ï¼Œä½†ä¸æ‡‰åœ¨æ–°æµç¨‹ä¸­ä½¿ç”¨ï¼‰"""
        print("è­¦å‘Š: _call_gemini_model (èˆŠç‰ˆ) è¢«èª¿ç”¨ã€‚")
        return self._call_gemini_model_with_retry(prompt) # Redirect to new retry logic

    def _format_response(self, analysis_text: str, symbol: str, timeframe: str) -> Dict[str, Any]:
        """æ ¼å¼åŒ–æ¨¡å‹å›æ‡‰"""
        try:
            cleaned_response = analysis_text.strip()
            if not cleaned_response or len(cleaned_response) < 50: # Basic check for empty/too short response
                cleaned_response = f"AIåˆ†æå›æ‡‰éçŸ­æˆ–ç„¡æ•ˆ ({symbol} {timeframe})ã€‚è«‹æª¢æŸ¥APIé…ç½®æˆ–é‡è©¦ã€‚åŸå§‹å›æ‡‰é•·åº¦: {len(analysis_text)}"
                status = "error_short_response"
            else:
                status = "success"

            return {
                "analysis_text": cleaned_response,
                "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "symbol": symbol,
                "timeframe": timeframe, # For N8N, this is "å¤šæ™‚é–“æ¡†æ¶"
                "status": status,
                "word_count": len(cleaned_response.split()) # More accurate word count
            }
        except Exception as e:
            print(f"æ ¼å¼åŒ–å›æ‡‰æ™‚å‡ºéŒ¯: {e}")
            return {
                "analysis_text": f"æ ¼å¼åŒ–åˆ†æçµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}",
                "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "symbol": symbol,
                "timeframe": timeframe,
                "status": "error"
            }